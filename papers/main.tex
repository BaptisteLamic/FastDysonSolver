\documentclass[aps,prb,reprint,amsmath,amssymb]{revtex4-2}
\usepackage{amsfonts}
\usepackage{dsfont}
\usepackage{cleveref}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{url} 
\usepackage{color}

\begin{document}
	
	\title{Solving the Transient Dyson Equation with Quasilinear Complexity via Matrix Compression}
	
	\author{Baptiste LAMIC}
	\affiliation{Univ. Grenoble Alpes, CEA, IRIG-Pheliqs, F-38000 Grenoble, France}
	
	\date{\today}
	
	\begin{abstract}
We introduce a numerical strategy to efficiently solve the out-of-equilibrium Dyson equation in the transient regime. By discretizing the equation into a compact matrix form and applying state-of-the-art matrix compression techniques, we achieve significant improvements in computational efficiency which results in quasi-linear scaling of both time and space complexity with propagation time. This enables to compute accurate solutions even for systems with multiple and disparate time scales. We benchmark our solver by simulating a voltage-biased Josephson junction formed by a quantum dot connected to two superconducting leads.

	\end{abstract}
	
	\maketitle
	
	\section{Introduction}
	
The quantum field formalism provides an elegant framework for describing interacting and many-body quantum systems by providing 
tools for evaluating $n$-point correlation functions, thus assessing the statistical properties of observables in both equilibrium and non-equilibrium regimes \cite{Altland2010, Rammer2007}. This formalism underpins various numerical schemes, including quantum determinant Monte Carlo \cite{Profumo2015, Kozik2010}, functional renormalization group methods \cite{Metzner2012} and perturbation theories \cite{Rammer2007, Altland2010}, all of which are capable of directly accessing quantities in the thermodynamic limit. However, these methods often require the evaluation of multidimensional integral operators, which makes their numerical resolution computationally expensive. 

The Dyson equation $G = g + g \cdot \Sigma \cdot G$ is one example of such integral equation. It relates the full Green function $G$ which describes the two-points correlations in the system, to the free Green function $g$ representing the non-interacting system, and the self-energy $\Sigma$ which accounts for interaction effects. Solving this equation has proven to be challenging both at equilibrium \cite{Dong2020} and out-of-equilibrium \cite{Talarico2019}, yet some significant advancements were made by leveraging matrix compression techniques \cite{Kaye2021, KayeEquilibrium}. In this work, we refine the usage of these compressions methods to solve the out-of-equilibrium Dyson equation with unprecedented efficiency. 
	
Within the real-time formulation of the Keldysh out-of-equilibrium field theory, the Dyson equation can be expressed as follows
	\begin{equation}
		\small
		G(t,t') =  g(t,t') +  \iint_{\boldmath{-\infty}}^{\infty}  g(t,t_1)  \Sigma(t_1,t_2) G(t_2,t') \text{d}t_1 \text{d}t_2 	\label{eq:formal_dyson_equation}.
	\end{equation}	
We select the basis of the Keldysh space such that the kernels can be written as
	\begin{equation}
		 G \equiv 
		\begin{pmatrix}
			0 & G^\text{A} \\
			G^\text{R} & G^\text{K}
		\end{pmatrix}, \quad
		 \Sigma \equiv \begin{pmatrix}
			\Sigma^\text{K} & \Sigma^\text{R} \\
			\Sigma^\text{A} & 0
		\end{pmatrix}, 
		\label{eq:basis:def}
	\end{equation}
	where the superscripts $\text{R}$, $\text{A}$, and $\text{K}$ denote the retarded, advanced, and kinetic components, as detailed in \cite{Rammer2007}.  Any retarded or advanced kernel $F^\text{R/A}$ satisfy
	\begin{align}
		\small
		F^\text{R}(t,t') &= \theta(t-t')F^\text{R}(t,t'), \\
		F^\text{A}(t,t') &= \theta(t'-t)F^\text{A}(t,t').
	\end{align}
	Eq.\,\ref{eq:formal_dyson_equation} can be written as
	\begin{align}
		\small
		G^\text{R} &= g^\text{R} +  g^\text{R} \Sigma^\text{R} G^\text{R}, \label{eq:retarded_dyson_equation} \\
		G^\text{A} &= {G^\text{R}}^\dagger, \\
		G^\text{K} &= \left(\mathds{1}+G^\text{R}\Sigma^\text{R}\right)g^\text{K}\left(\mathds{1} + \Sigma^\text{A} G^\text{A}\right) + G^\text{R} \Sigma^\text{K} G^\text{A}, \label{eq:kinetic_dyson}
	\end{align}
	where all the integrations are implicit. We assume that the problem's orbital degrees of freedom have been discretized so that for any pair of kernels $(F,R)$, the product $F(t,t_1) R(t_1,t')$ satisfies:
	\begin{equation}
		\left[ F(t,t_1) R(t_1,t') \right]_{p,q} = \sum_{k} 	F_{p,k}(t,t_1) R_{k,q}(t_1,t').
	\end{equation}
	A natural starting point to solve \cref{eq:retarded_dyson_equation} is to discretize the time axis using a regular time grid $t_p = t_0 + p \delta_t$ for $0 \leq p \leq N$, where $\delta_t$ is the time step and $N+1$ is the number of points. Any kernel $F(t,t')$ is formally discretized into the block matrix $\mathbf{F}$. Each block is defined by
	 \begin{equation}
		\mathbf{F}_{i,j} \equiv F(t_i,t_j). 
	\end{equation}
	The traditional method stores each block explicitly, thus requiring $\mathcal{O}(N^2)$ space, and setting a lower bound on the computational complexity.  The retarded Dyson equation is usually solved using a time-stepping approach demanding $\mathcal{O}(N^3)$ operations as $N$ steps must be computed for $N$ values of $t'$ and each step requires summing over the full system history to evaluate the integral as shown in \cite{Talarico2019, Kaye2021}. Using \cref{eq:kinetic_dyson} to evaluate the  Keldysh component requires integrating over all the history of the system, starting at $-\infty$.  While there
	are methods to perform this integral \cite{NESSI, Kaye2021, Dong2020}, we impose in the following that the self-energy is turned on at $t = t_0$ and eventually turned off after $t_\text{end}$, \textit{i.e.} $\forall (t,t') \notin [t_0, t_\text{end}]^2: \Sigma(t,t')^{R/A/K} = 0$. This simplifies the discussion by ensuring that all integration domains lie within $[t_0, t_\text{end}]$. 


As $N \sim \tau_{\text{long}} / \tau_{\text{short}}$, with $\tau_{\text{long}}$ and $\tau_{\text{short}}$ the longest and shortest system timescales, the traditional time-stepping method require $\mathcal{O}(\left(\tau_{\text{long}} / \tau_{\text{short}}\right)^3)$ operations. This unfavourable scaling has prevented the use of the Dyson equation in simulating quantum system with widely separated timescales. Alternative methods that sacrifice part of the quantum field elegance and flexibility for speed have emerged \cite{kloss2021tkwant, kloss2021tkwant, Tuovinen}, or solver specialized for specific form of the equation \cite{Ortega-Taberner2023, Venitucci2018, KayeEquilibrium}. However, recent works have reduced the required time and space to solve the Dyson equation  to $\mathcal{O}(N^2 \log(N))$ and $\mathcal{O}(N \log(N))$, respectively, by using matrix compression techniques to perform fast integral evaluation and compress the kernel representation \cite{Kaye2021}. Here, we take a step beyond  by proposing a new discretization of the Dyson equation in time domain that enables a more efficient use of matrices compression method. As a result, we reduce the time complexity from a quadratic scaling to the quasilinear scaling $\mathcal{O}(N \log(N))$ without further assumptions. 
	
\section{Algorithm}
We discretize the kernel products using the Nyström method with quadrature weights that remain constant away from the integration domain edges, a condition satisfied by Gregory quadratures \cite{Fornberg2019}. To avoid overwhelming the reader with indices, we propose here a compact formulation of the algorithm and we restrict the presentation to a low-order discretization using trapezoidal quadrature. An expanded version is presented in the supplemental materials \cite{Supplement}. Thus, we approximate the kernel product $A^\text{R} B^\text{R}$ as
\begin{equation}
	 (A^\text{R} B^\text{R})(t_p,t_q) \approx 	\mathbf{A^\text{R}}  * \mathbf{B^\text{R}} ,
\end{equation}
where we defined the discretized Kernel product as
		\begin{equation}
		\mathbf{A^\text{R}}  *\mathbf{B^\text{R}} \equiv \frac{\delta_t}{2} \sum_{k = q}^{p} (2 - \delta_{p,k} -  \delta_{k,q})\mathbf{A^\text{R}}_{p,k} \mathbf{B^\text{R}}_{k,q}.
	\end{equation}
	This can be rewritten as 
	\begin{equation}
		\small
		\frac{ \mathbf{A^\text{R}}  *\mathbf{B^\text{R}}}{\delta_t} = (\mathbf{w} \odot \mathbf{A^\text{R}}) \cdot (\mathbf{w} \odot \mathbf{B^\text{R}}) +  \frac{\text{Dg}\left( \mathbf{A^\text{R}} \right) \text{Dg}\left( \mathbf{B^\text{R}} \right) }{4},
	\end{equation}
	where $\odot$ is the block-wise matrix product defined as $\left [ A \odot B \right ]_{p,q} \equiv A_{p,q} B_{p,q}$ with $p$, $q$ block indices.  $\text{Dg}\left( \mathbf{M} \right)$ is the block-diagonal matrix defined by  
	$\text{Dg}\left( \mathbf{M}\right)_{p,q} \equiv  \mathbf{M}_{p,q} \delta_{p,q}$,
	and $\mathbf{w}$  is the block matrix with entries:  $ [(1 - \delta_{p,q} /2 )]_{p,q}$.
	 Similar expressions of the discretized kernel product can be derived for the other relevant products, \text{i.e.} $A^\text{A} B^\text{A}$, $A^\text{K} B^\text{K}$, $A^\text{K} B^\text{R}$ and $A^\text{A} B^\text{K}$.  We can now turn to the matrix formulation of the discretized Dyson equation
	\begin{equation}
		G^\text{R} = g^\text{R} +F^\text{R} G^\text{R},
	\end{equation}
	where $F^\text{R} = g^\text{R} \Sigma^\text{R}$. Using the above results, we recast this equation as: 
\begin{align}
	\mathbf{w} \odot \mathbf{G^\text{R}} &= \mathbf{S} + \mathbf{g^\text{R}} + \delta_t \cdot (\mathbf{w} \odot \mathbf{F^\text{R}}) \cdot (\mathbf{w} \odot \mathbf{G^\text{R}}) \label{eq:dyson:compact}, \\
	\qquad \text{with }
	\mathbf{S} &\equiv \delta_t \cdot \left(\frac{\text{Dg}\left( \mathbf{F^\text{R}} \right)}{4} - \frac{1}{2}\right) \cdot \text{Dg}\left( \mathbf{G^\text{R}} \right).
\end{align}
Due to the causal nature of this equation, the diagonal terms of $\mathbf{G^\text{R}}$ can be quickly deduced from the diagonal elements of $\mathbf{F^\text{R}}$ and $\mathbf{g^\text{R}}$; hence, the terms $\mathbf{S}$ are known before solving for $\mathbf{G^\text{R}}$. As a result, this equation can be efficiently solved using matrix algebra methods.
%\subsubsection{Matrix compression}
\begin{table}[]
	\begin{tabular}{@{}llll@{}}
		\toprule
		& Operation  & \centering HSS matrix   & Dense matrix\\ \midrule
		& Direct compression  & $\mathcal{O}(r N^2)$ &  \\
		& Stochastic compression & $\mathcal{O}(r^2 N)$    \\
		& $A B$ &  $\mathcal{O}(r^2 N)$  & $\mathcal{O}(N^3)$  \\
		& $A \setminus B$ &  $\mathcal{O}(r^2 N)$  &    $\mathcal{O}(N^3)$ \\ 
		& $A + B$ &  $\mathcal{O}(r^2 N)$   & $\mathcal{O}( N^2)$    \\  \bottomrule
	\end{tabular}
	\caption{Time complexity of several matrix operations. $A$ and $B$ are $N \times N$ matrices, $r$ is their maximum HSS-rank of the involved matrices. For a review of HSS algorithms, see \cite{Massei2020}. \label{tab:hss:complexity}}
\end{table}

Eq.\,(\ref{eq:dyson:compact}) enables the direct use of matrix compression techniques. The success of the \emph{equation-of-motion} techniques, as well as recent development such as \cite{Tuovinen}, suggests that the Green function can often be well-approximated as the solution of a differential equation. By discretizing differential operators into banded matrices using an appropriate basis, it becomes evident that the inverse of such operators possesses low-rank off-diagonal blocks \cite{Vandebril2005}.
This property has received empirical validation from \cite{Kaye2021}.

Such matrices can be efficiently represented using recursive factorization methods that separate short and long timescales by splitting the matrix into diagonal and off-diagonal blocks. The off-diagonal blocks, which capture long timescales, are compressed with low-rank factorization techniques. Among the various compression methods that apply such strategy, we choose the \emph{hierarchically semi separable (HSS) }  factorization \cite{Chandrasekaran2006, Martinsson2011, Xia2010, Massei2020} using the \emph{Julia} package \emph{HssMatrices.jl} \cite{Bonev2021HssMatrices}.
The complexity of HSS matrix operations depends on both the matrix size and their HSS-ranks $r$, which is the maximal rank of the off-diagonal blocks, see tab.\,\ref{tab:hss:complexity}. After compression, most of the matrix algebra operations can be performed in $\mathcal{O}(r^2 N)$ space and time, significantly reducing the computational time and memory requirements when $r \ll N$. 
However, the direct compression of a matrix $A$ into its factorization $A_\text{HSS}$ cost of $\mathcal{O}(r N^2)$ operations. 
This bottleneck can be bypassed using stochastic compression methods such as the one proposed by \cite{Martinsson2011}. Supposing that any matrix entry can be evaluated in $\mathcal{O}(1)$, and that the matrix product $Ax$ and $A^\dagger x$ for any vector $x$ can be performed in $T_\text{mul}$ operations, the compression complexity is reduced to $\mathcal{O}(r T_\text{mul} + r^2 N)$ while mostly preserving the accuracy. Hence, when $T_\text{mul} = \mathcal{O}(N)$, the compression reaches $\mathcal{O}(r^2 N)$ complexity in time. 
%However, there is a controlled but non-zero probability that the compression may  fail due to its stochastic nature \cite{Martinsson2011}. 
Lastly, the element-wise product $(\mathbf{w} \odot \mathbf{A^\text{R}})$ can be performed either using a general element-wise product algorithm for HSS-matrices \cite{Massei2020} or by exploiting the fact that most of the elements of $\mathbf{w}$ are equal to one. The complexity of the resolution of the Dyson equation, including the factorization stage, is thus $\mathcal{O}(rT_\text{mul} + r^2N)$.

Implementing fast multiplications $Ax$ and $A'x$ requires exploiting the structure of the uncompressed matrix. Discretized stationary kernels are represented as block-Toeplitz matrices. Hence, the \emph{Fast Fourier Transform} allows for multiplications in $\mathcal{O}(N\log N)$, resulting in a compression complexity of $\mathcal{O}(rN\log N)$. When the matrices are sparse, a complexity of $\mathcal{O}(r^2N)$ is readily achieved. For kernels that are solutions of a differential equation, the sparse representation of the differential operator can be compressed and solved, achieving optimal complexity.
Therefore, a quasi-linear complexity is reached as long  as $r$ grows slowly enough with the problem size. 

%We should note that the HSS representation $A_\text{HSS}$ of a matrix $A$ is rarely exact, yet truncation errors are controlled and can be made negligible compared to the discretization error. 

\section{benchmark}

\begin{figure}
	\includegraphics[width=8.6cm]{ SQDS_benchmark_varying_t_end.pdf}
	\caption{
		Solver benchmark for fixed time step $\delta_t$ with varying time horizon $t_\text{end}$ and voltage bias. The constant symmetric voltage  bias ensures that $\phi(t_{\text{end}}) = 16\pi$ with $\phi(0) = 0$.    The benchmark parameters are $\Gamma_L = \Gamma_R = 5 \Delta / \hbar$, $\delta_t = 0.025  h/\Delta, \beta = 10^2 \Delta^{-1}$. 
		\emph{a)} Solver runtime with and without compression.
		\emph{b)} HSS-ranks of the discretized full Green function.
		\emph{c)} Average current flowing from the dot to the right lead for various applied voltages.
		\label{fig:benchmark:fixed_dt} 
	}
\end{figure}

\begin{figure}
	\includegraphics[width=8.6cm]{ SQDS_benchmark_varying_N.pdf}
	\caption{
		Solver benchmark for fixed time horizon $t_\text{end} = 3.2 h / \Delta$ with varying time step $\delta_t$.  The simulation parameters are: $\Gamma_L = \Gamma_R = 5 \Delta / \hbar$, $\beta = 10^2 \Delta^{-1}$. The constant voltage bias applied across the junction enforce that $\phi(t_{\text{end}}) = 16\pi$ with $\phi(0) = 0$. 
		\emph{a)} Solver runtime with and without compression.
		\emph{b)} HSS-ranks of the discretized full Green function.
		\emph{c) } Normalized error $\mathcal{E}$ in the current estimate as a function of $\delta_t$. 
		\emph{d)} Variation of the HSS-rank when varying the required accuracy $\epsilon = \epsilon_\text{rel} = \epsilon_{abs}$ of the HSS-representation for $\delta_t =  3.1\cdot 10^{-3} h/\Delta$.
		\label{fig:benchmark:convergence} 
	}
\end{figure}

We benchmarked this algorithm by simulating the out-of-equilibrium transport in a Josephson junction formed by a quantum dot (QD) connected to two superconducting leads as this system is well known \cite{Souto2020, Jacquet2020,Yeyati1997,Rogovin1974}. The solver implementation is hosted on github \cite{FastSolver}. All the simulations were performed on a ultra 265K CPU with 48GB of RAM. The leads are superconductors described by the \emph{Barden-Cooper-Schrieffer} theory \cite{Tinkham2004} with a uniform real and positive order parameter $\Delta$. The operators $\psi_{\alpha,l,s}^\dagger$ create an electron of spin $s$ in mode $\alpha$ of lead $l$ with energy $ \varepsilon_{\alpha,l}$, while $d_s^\dagger$ creates an electron of spin $s$ on the dot with energy $\varepsilon_d$. $t_{\alpha,l}(t)$ represents the tunnel coupling between the dot and mode $\alpha$ of lead $l$. Using the usual Nambu spinor, where $d \equiv (d_\uparrow, d_\downarrow^\dagger)^T$ and $\Psi_{\alpha, l} \equiv ( \psi_{\alpha,l,\uparrow} , \psi_{\alpha,l,\downarrow}^\dagger )^T$, we can write the standard Hamiltonian as
$
 	H(t) = H_{L} + H_{D} + H_{T}(t)
$ \cite{Jacquet2020,Yeyati1997,Rogovin1974} , 
where
\begin{align}
	H_L
	&= \sum_{\alpha,l} \Psi_{\alpha,l}^\dagger \left(\sigma_x \Delta + \sigma_z \varepsilon_{\alpha,l}\right) \Psi_{\alpha,l},
	\\
	H_D &= d^\dagger \sigma_z \varepsilon_d d,
	\\
	H_T(t) &= \sum_{\alpha,l} \left\{ \Psi_{\alpha,l}^\dagger  \mathcal{T}_{\alpha,l}(t)  d + \text{H.c} \right\},
\end{align}
with $\mathcal{T}_{\alpha,l} \equiv t_{\alpha,l} \sigma_z e^{i\sigma_z \phi_{\alpha,l}(t)/2}$ being the time-dependent tunnel matrix, $\sigma_i$ the Pauli matrices acting in the Nambu space, the transmission phase $\phi_l(t) \equiv \frac{2e}{\hbar} \int^t{V_l(\tau)} d\tau $, $-e$ the charge of an electron and $V_l(\tau)$ the voltage bias from the QD to the lead $l$.  Employing the quantum field formalism \cite{Rammer2007, Altland2010}, we integrate out the infinite leads, resulting in an effective description of the dot in terms of its bare Green function $g_d$ dressed by the self-energies $\Sigma_l$ induced by the leads
\begin{equation}
	\Sigma_l = \hbar^{-1}\sum_{\alpha, \alpha'}\mathcal{T}_{\alpha,l}(t)^\dagger \cdot g_{(\alpha,l),(\alpha',l)}(t,t') \cdot \mathcal{T}_{\alpha',l}(t')^\dagger.
\end{equation}
In the flat-band limit, and upon absorbing the density of states into the tunneling coefficient, the  retarded Green functions of the leads are
\begin{equation}
	g_{\alpha}^{R}(t)
	= -i  \pi \delta(t) \sigma_0 + \pi  \Delta \left\{J_0( \frac{\Delta t}{\hbar} )\sigma_x + i J_1( \frac{\Delta t}{\hbar}) \sigma_0 \right\}, 
\end{equation} 
with $J_k$ the Bessel functions and $\sigma_0$ the identity matrix in the Nambu space. The dot's retarded Green function is
\begin{equation}
	g_{\text{dot}}^\text{R}(t) = -i e^{- i\sigma_z \varepsilon_d   t / \hbar }. 
\end{equation}
The average electric current flowing from the dot to the lead $l$ is \cite{Jacquet2020}
\begin{multline}
	\left \langle I_l(t) \right \rangle =  \frac{e }{ 2 \hbar} \text{tr}^{NK}\left\{ \tau_z \sigma_z
	\int \left[
	G(t,\tau) \Sigma_l(\tau,t) 	\right. \right.  \\ 
 \left. \left.	- \Sigma_l(t,\tau) G(\tau,t) \right] \text{d} \tau 
	\right\}
\end{multline}
where $G$ is the full Green function of the dot, $\text{tr}^\text{NK}$ is the trace over the Keldysh and Nambu indices, and $\tau_z$ is an operator acting on the Nambu space. Using the same Keldysh space basis as \cref{eq:basis:def}, its matrix representation is
\begin{equation}
	\tau_z =\frac{1}{2}\begin{pmatrix}
		1 & 2 \\
		2 & 3
	\end{pmatrix}.
\end{equation}

The evaluation of $\left \langle I_l(t) \right \rangle $ is performed using the fast kernel product described above; hence, it is deduced in $\mathcal{O}(r^2 N)$ from the full Green function. We suppose that the bare QD is half filled, \textit{i.e.} $g_{\text{dot}}^\text{K}(t) = 0$, and that the leads are initially at thermal equilibrium, hence the bare lead Keldysh Green functions satisfy
\begin{equation}
 g_{l}^\text{K} = g_{l}^\text{R} \cdot \rho_\beta - \rho_\beta  \cdot {g_{l}^\text{A}}, 
\end{equation}
	where $\rho_\beta(t,t')  \equiv -i \beta^{-1}  \text{csch} \left\{ \pi (t-t') \beta^{-1}\right\}$ and $\beta \equiv 1/k_B T$. We name the two superconductor leads left ($L$) and right ($R$).  For $t \geq 0$, the junction is drive out-of-equilibrium by symmetric voltage bias, \text{i.e.} $V_{L/R}(t) = \pm V/2 $. The phase difference $\phi(t) = \phi_R(t) - \phi_L(t)$ satisfies $\phi(0) = 0$.  We neglect any dependence of the tunnel coupling on the mode index $t_{\alpha,l} = t_{\alpha}$ and the tunnelling rate associate to the lead $l$ is $\Gamma_{l} =  \pi |t_{l}|^2/\hbar$. 
Such Josephson junctions host a pair of discrete particle-hole symmetric bound states, known as Andreev bound states (ABS), which are entirely detached from the continuum outside the superconducting gap and exhibit avoided crossing behaviour at the Fermi energy  when the dot is not resonant. Each of these states carries an opposite current \cite{Yeyati2003, Yeyati1997}.   We set $\Gamma_{\alpha} = 5\Delta / \hbar$, $\beta = 10^2 \Delta^{-1}$ and $ \epsilon_d = 0$.

For the first set of experiments, we impose $\phi(t_{\text{end}}) = 16\pi$  by setting $V = 2\hbar \phi(t_{\text{end}})/ e t_{\text{end}}$.  Thus, we have $\phi(t) = 16\pi t / t_{\text{end}}$. 
We measure the solver runtime for increased simulation length $t_\text{end}$, keeping $\delta_t$ constant as shown in  \cref{fig:benchmark:fixed_dt}. At $\phi \ll 1$ the states are symmetrically occupied, and the current vanishes. However, the voltage bias induces non-adiabatic transitions that couple the ABSs to each other and to the continuum outside the superconducting gap, driving the system into a non-stationary steady state \cite{Yeyati2003, Yeyati1997, BLamic2020}. In the steady state, the ABSs occupation is not symmetric, hence a net current flow across the junction. The HSS-rank of the retarded Green function saturates, while the one of the kinetic Green function grows slowly enough to ensure quasi-linear solving time in $t_{\text{end}} / \delta_t$.

 The simulation error arises from the interplay between discretization error and compression error. When the compression tolerance is sufficiently small, the error is dominated by the discretization error $\epsilon(\delta_t)$, which can be assumed to have the form
 	\begin{equation}
 		\epsilon(\delta_t) = \sum_{p \geq \alpha + 1} a_p \, \delta_t^p + o(\delta_t^n),
 	\end{equation}
 	where $\alpha \in \mathbb{N}^+$ is the convergence rate. By performing $n_\text{sim}$ simulations with varying time steps $\delta_{t, 0 < i \leq n_\text{sim}}$, one can evaluate $n_\text{sim} - 1$ terms of this expansion and  
construct a simulation result whose discretization error $\epsilon(\delta_{t,1}, ..., \delta_{t,n})$ is of the form 
 	  \begin{equation}
 	 	\epsilon(\delta_{t,1}, ..., \delta_{t,n}) = \sum_{p \geq \alpha + n_\text{sim}} a_p' \delta_t^p  + o(\delta_t^n),
 	 \end{equation}
 Hence, this increases the method convergence rate from $\alpha$ to at least $\alpha + n_\text{sim}$. This classical method is known as Richardson acceleration \cite{numericalRecipies}.
 To evaluate the solver convergence rate, we first compute a high-accuracy reference current estimate $\left\langle I_\text{ref} \right\rangle(t)$ using Richardson acceleration
 	\begin{equation}
 		\left\langle I_\text{ref} \right\rangle(t) \equiv \frac{\left\langle I_{\delta^*_t}\right\rangle(t) - 6 \left\langle I_{\delta^*_t /  2}\right\rangle(t)  + 8  \left\langle I_{\delta^*_t / 4}\right\rangle(t)} {3},
 	\end{equation}
 	where $\delta^*_t = 3.9\cdot 10^{-4}\Delta / \hbar$ and  $\left\langle I_{\delta_t}(t)\right\rangle$  the current evaluated for the time step $\delta_t$.  By construction, this estimate as a convergence rate of at least $3$ in $\delta^*_t$.
We then define the convergence error  for $\delta_t > \delta_t^*$ as
\begin{equation}
\mathcal{E}\left( \delta_t \right) \equiv \frac{\int|\left\langle I_{\delta_t}(t)\right\rangle-\left\langle I_\text{ref}(t)\right\rangle|^2\text{d}t}{\int|\left\langle I_\text{ref}(t)\right\rangle|^2\text{d}t}, 
\end{equation}
the evolution of this quantity with $\delta_t$ is shown in subplot \emph{c)} of \cref{fig:benchmark:convergence}, from which we extract a convergence rate $\alpha = 1$, \emph{i.e.} $\mathcal{E} \propto \delta_t^{-1}$. Although a first-order exact quadrature typically induces a second-order convergence rate, we attribute this slower convergence to the discontinuity of $\rho_\beta$. It can be overcome by using Richardson acceleration, or by improving the quadrature. We also observe when $\delta_t$ is reduced that the HSS-rank of $G^\text{R}$ remains constant, while the one of $G^\text{K}$ grows slowly enough to maintain quasi-linear complexity, see \cref{fig:benchmark:convergence}\emph{.a)} and \cref{fig:benchmark:convergence}\emph{.b)}. Using the same setup, we evaluate the algorithm dependency on the absolute tolerance $\epsilon_\text{abs}$ and the relative tolerance $\epsilon_\text{rel}$ use for the HSS compression. These parameters act as cutoffs in the selection of the singular values of the off diagonal blocks. Increasing the tolerance results in a decrease in the HSS ranks, thereby speeding up the solver. They should be chosen small enough to not penalize the solving accuracy while being large enough to not needlessly slow down the solver. In \cref{fig:benchmark:convergence}\emph{.d)}, we observe the dependence of HSS-rank with respect to $\epsilon_\text{abs}$ and $\epsilon_\text{rel}$. The exponential convergence of the error as the HSS-rank increases shows that compression can be used even when high accuracy is needed.

\begin{figure}
	\centering
	\includegraphics[width=8.6cm]{IV.pdf}
	\caption{
		I-V characteristics of a voltage-biased quantum-dot Josephson junction with symmetric coupling $\Gamma = \Gamma_L = \Gamma_R$ at resonance $\varepsilon = 0$. Transient simulations run from $t = 0$ to $t_\text{end} = \max(200 h/\Delta, 20 T_\text{J})$ with compression tolerances $\epsilon_\text{rel} = \epsilon_\text{abs} = 10^{-6}$ in simulation units. Results reproduce the classical behaviour from.~\cite{Yeyati1997}.
		\label{fig:steady_state} 
	}
\end{figure}

We extract the low-frequency averaged current, $I_\text{LF}(t)$, by averaging the instantaneous current $\langle I_l(t) \rangle$ over two Josephson periods, $2 T_\text{J} = 2\pi / (e V \hbar)$, accounting for the fractional Josephson radiation characteristic of a quantum dot Josephson junction at resonance \cite{BLamic2020}
\begin{equation}
	I^{\text{LF}}_{\delta_t}(t) \equiv \frac{1}{2 T_\text{J}} \int_{t - 2 T_\text{J}}^{t} \langle I_{\delta_t}(t') \rangle\, dt'.
\end{equation}
 The system is simulated from to $t_0 = 0$ to $t_\text{end} = \max(200\, \hbar/\Delta,\ 20\, T_\text{J})$
 to accommodate the different timescales across voltage regimes. At low voltage, the longest timescale is the Josephson period $T_\text{J}$, where interactions between the ABSs and the continuum occur primarily at anti-crossings, requiring simulation over many Josephson periods. At higher bias, however, the stronger couplings allow the system to reach steady state more rapidly, eliminating the need for such extended simulations. 
We use an adaptive multi-step Richardson extrapolation to both increase the result accuracy and provide an error estimate. The implementation is part of the solver module see \cite{FastSolver}.  The step size is refined, until the absolute or relative error estimate are below $10^{-2}$ in simulation units. The initial time step is set to $\min(0.3 T_\text{J}, 0.5 /\Gamma )$ to ensure proper resolution of the fast timescale.
 
As the ABSs are separated from the continuum by a finite gap $\Delta_\text{ABS}$. For $V \ll f(\Delta_\text{ABS})$, the ABSs become effectively decoupled from the continuum \cite{BLamic2020, Yeyati2003}. In the absence of relaxation mechanisms, their long-time occupations depend on initial conditions, implying no well-defined steady state exists. Nevertheless, $I_\text{LF}(t)$ rapidly approaches a stationary regime, enabling extraction  of meaningful current-voltage characteristics even at low voltages. This behavior is illustrated in \cref{fig:steady_state}, which reproduces the classical results from \cite{Yeyati1997}.

	\section{Conclusion}
	
		By carefully crafting a discretization of the Dyson equation and by using modern matrix compression codes, we solved the out-of-equilibrium Dyson equation in quasi-linear time and space. Benchmarks demonstrate quasilinear scaling, with the measured runtime exhibiting $\mathcal{O}(N \log N)$ behavior when using \emph{HSS} compression, in contrast to the expected $\mathcal{O}(N^3)$ trend observed in reference simulations.  Utilizing a desktop, we accessed parameter regimes that are typically beyond the reach of a single-CPU machine. 
		This quasi-linear complexity critically depends on the effectiveness of HSS compression in representing the Green functions and self-energies, which we expect to generally hold, as discussed earlier. Consequently, given the existence of scalable and efficient implementations of HSS compression or similar methods, and considering their robustness, we anticipate that this solving technique will be capable of addressing extremely large problems, provided that the compressed representations of the kernels fit in memory.
		
		This strategy might be extended to solve quantum field equations involving higher-order correlation functions, whose resolution has so far been impeded by the absence of efficient methods. Such an extension would pave a new way for the simulation of interacting many-body systems far from equilibrium in the transient regimes.
		
	\section*{Acknowledgments}
	
	The author would like to thank Manuel Houzet and Julia S. Meyer for their valuable feedback, and Cécile X. Yu for her proofreading and support.
	
	\bibliography{biblio}
	
\end{document}
